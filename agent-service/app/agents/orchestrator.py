"""
Orchestrator Agent - LLM é©±åŠ¨çš„è‡ªä¸»ç¼–æ’è€…

ä½¿ç”¨ ReAct æ¨¡å¼ï¼š
- LLM æ€è€ƒå½“å‰çŠ¶æ€
- LLM å†³å®šä¸‹ä¸€æ­¥æ“ä½œ
- æ‰§è¡Œæ“ä½œï¼Œè·å–ç»“æœ
- LLM åˆ†æç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥
- é‡å¤ç›´åˆ° LLM å†³å®šå®Œæˆ
"""
from typing import Dict, Any, Optional, List
from loguru import logger
import time
import json
import re
from dataclasses import dataclass

from app.agents.base import BaseAgent
from app.services.llm import LLMService, LLMProvider
from app.services.llm.adapters.base import LLMMessage
from app.core.agent_registry import agent_registry
from app.core.graph_controller import agent_graph_controller
from app.services.rust_client import rust_client
from app.core.audit_phase import AuditPhaseManager, AuditPhase, get_phase_manager
from app.core.monitoring import get_monitoring_system
from app.core.resilience import get_llm_circuit, get_llm_rate_limiter, with_retry, LLM_RETRY_CONFIG


@dataclass
class AgentStep:
    """æ‰§è¡Œæ­¥éª¤"""
    thought: str
    action: str
    action_input: Dict[str, Any]
    observation: Optional[str] = None
    sub_agent_result: Optional[Any] = None


class OrchestratorAgent(BaseAgent):
    """
    ç¼–æ’ Agent - ReAct æ¨¡å¼

    LLM å…¨ç¨‹å‚ä¸å†³ç­–ï¼š
    1. LLM æ€è€ƒå½“å‰çŠ¶æ€
    2. LLM å†³å®šä¸‹ä¸€æ­¥æ“ä½œ
    3. æ‰§è¡Œæ“ä½œï¼Œè·å–ç»“æœ
    4. LLM åˆ†æç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥
    5. é‡å¤ç›´åˆ° LLM å†³å®šå®Œæˆ
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(name="orchestrator", config=config)

        config = config or {}
        self._llm_config = config
        self._llm: Optional[LLMService] = None

        self.max_iterations = config.get("max_iterations", 20)
        self._conversation: List[Dict[str, Any]] = []
        self._steps: List[AgentStep] = []
        self._all_findings: List[Dict[str, Any]] = []

        # è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
        self._runtime_context: Dict[str, Any] = {}

        # è·Ÿè¸ªå·²è°ƒåº¦çš„ Agent ä»»åŠ¡ï¼Œé¿å…é‡å¤è°ƒåº¦
        self._dispatched_tasks: Dict[str, int] = {}

        # ä¿å­˜å„ä¸ª Agent çš„å®Œæ•´ç»“æœ
        self._agent_results: Dict[str, Dict[str, Any]] = {}

        # è¿›åº¦è·Ÿè¸ª
        self._progress: int = 0

        # é›†æˆå®¡è®¡é˜¶æ®µç®¡ç†
        self._phase_manager: Optional[AuditPhaseManager] = None
        self._monitoring = get_monitoring_system()

        # å®¹é”™æœºåˆ¶
        self._llm_circuit = get_llm_circuit()
        self._llm_rate_limiter = get_llm_rate_limiter()

    def _update_progress(self, progress: int, message: str = ""):
        """æ›´æ–°å®¡è®¡è¿›åº¦"""
        self._progress = min(100, max(0, progress))
        if message:
            logger.info(f"[Orchestrator] è¿›åº¦: {self._progress}% - {message}")

        # å‘å¸ƒè¿›åº¦äº‹ä»¶åˆ°å‰ç«¯
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡æ¥å‘å¸ƒäº‹ä»¶ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        import asyncio

        async def _publish_progress():
            try:
                await self._publish_event("progress", {
                    "progress": self._progress,
                    "message": message
                })
            except Exception as e:
                logger.warning(f"[Orchestrator] å‘å¸ƒè¿›åº¦äº‹ä»¶å¤±è´¥: {e}")

        # å¦‚æœåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ï¼Œç›´æ¥åˆ›å»ºä»»åŠ¡
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(_publish_progress())
        except:
            pass

    @property
    def llm(self):
        """å»¶è¿Ÿåˆå§‹åŒ– LLM æœåŠ¡"""
        if self._llm is None:
            try:
                provider_str = self._llm_config.get("llm_provider", "anthropic")
                try:
                    provider = LLMProvider(provider_str)
                except ValueError:
                    logger.warning(f"æœªçŸ¥çš„ LLM provider '{provider_str}'ï¼Œä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼")
                    provider = LLMProvider.OPENAI

                self._llm = LLMService(
                    provider=provider,
                    model=self._llm_config.get("llm_model", "claude-3-5-sonnet-20241022"),
                    api_key=self._llm_config.get("api_key"),
                    base_url=self._llm_config.get("base_url"),
                )
            except Exception as e:
                logger.error(f"LLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                raise ValueError("LLM æœåŠ¡æœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½® API Key")
        return self._llm

    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®¡è®¡ç¼–æ’"""
        audit_id = context.get("audit_id")
        self.think(f"å¼€å§‹ç¼–æ’å®¡è®¡ä»»åŠ¡: {audit_id}")

        try:
            return await self._execute_with_llm(context)
        except Exception as e:
            logger.error(f"å®¡è®¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return {
                "agent": self.name,
                "status": "error",
                "error": str(e),
                "thinking_chain": self.thinking_chain
            }

    async def _execute_with_llm(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """LLM é©±åŠ¨çš„è‡ªä¸»ç¼–æ’ - ReAct æ¨¡å¼"""
        audit_id = context["audit_id"]
        project_id = context["project_id"]
        start_time = time.time()

        # åˆå§‹åŒ–é˜¶æ®µç®¡ç†å™¨
        self._phase_manager = get_phase_manager(audit_id)

        # æ³¨å†Œ Orchestrator
        orchestrator_id = f"orchestrator_{audit_id}"
        self.agent_id = orchestrator_id
        await agent_registry.register_agent(
            agent_id=orchestrator_id,
            agent_name="Orchestrator",
            agent_type="orchestrator",
            task=f"ç¼–æ’å®¡è®¡: {audit_id}",
            parent_id=None,
            agent_instance=self,
        )

        # ä¿å­˜è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
        self._runtime_context = {
            "audit_id": audit_id,
            "project_id": project_id,
            "project_path": context.get("project_path", ""),
            "audit_type": context.get("audit_type", "quick"),
            "config": context.get("config", {}),
        }

        # åˆå§‹åŒ–è¿›åº¦
        self._progress = 0
        self._update_progress(5, "åˆå§‹åŒ–å®¡è®¡ä»»åŠ¡")

        # åˆå§‹åŒ–å®¡è®¡é˜¶æ®µ
        await self._phase_manager.transition_to(AuditPhase.INITIALIZATION)
        await self._publish_event("thinking", {
            "message": f"å®¡è®¡é˜¶æ®µ: {self._phase_manager.current_phase.value}"
        })

        # æ„å»ºåˆå§‹æ¶ˆæ¯
        system_prompt = self._get_system_prompt()
        initial_message = self._format_initial_message(context)

        # åˆå§‹åŒ–å¯¹è¯å†å² - ä½¿ç”¨ LLMMessage å¯¹è±¡
        self._conversation = [
            LLMMessage(role="system", content=system_prompt),
            LLMMessage(role="user", content=initial_message),
        ]

        self._steps = []
        self._all_findings = []
        self._agent_results = {}
        self._dispatched_tasks = {}
        final_result = None

        # åˆå§‹åŒ–é”™è¯¯è®¡æ•°å™¨
        self._empty_response_count = 0
        self._format_error_count = 0

        self.think("Orchestrator Agent å¯åŠ¨ï¼ŒLLM å¼€å§‹è‡ªä¸»ç¼–æ’å†³ç­–...")
        await self._publish_event("thinking", {
            "message": "Orchestrator Agent å¯åŠ¨ï¼Œå¼€å§‹å®¡è®¡ç¼–æ’..."
        })

        try:
            # è½¬æ¢åˆ°è§„åˆ’é˜¶æ®µ
            await self._phase_manager.transition_to(AuditPhase.PLANNING)

            for iteration in range(self.max_iterations):
                self._iteration = iteration + 1
                logger.info(f"[Orchestrator] Iteration {iteration + 1}/{self.max_iterations}")

                # è°ƒç”¨ LLM è¿›è¡Œæ€è€ƒå’Œå†³ç­–ï¼ˆå¸¦å®¹é”™æœºåˆ¶ï¼‰
                try:
                    logger.debug(f"[Orchestrator] å‘é€ LLM è¯·æ±‚ï¼Œå½“å‰å¯¹è¯å†å²é•¿åº¦: {len(self._conversation)}")

                    # åº”ç”¨é€Ÿç‡é™åˆ¶
                    await self._llm_rate_limiter.acquire()

                    # ä½¿ç”¨ç†”æ–­å™¨ä¿æŠ¤ LLM è°ƒç”¨
                    async def _llm_call():
                        return await self.llm.generate(messages=self._conversation)

                    response = await self._llm_circuit.call(_llm_call)
                    llm_output = response.content if hasattr(response, 'content') else ""

                    # è®°å½• LLM è°ƒç”¨æŒ‡æ ‡
                    await self._monitoring.record_llm_call(
                        model=self._llm_config.get("llm_model", "unknown"),
                        tokens_used=len(llm_output.split()),  # ç²—ç•¥ä¼°è®¡
                        duration=0.1,  # TODO: å®é™…æµ‹é‡
                        success=True,
                    )

                    logger.info(f"[Orchestrator] LLM å“åº”é•¿åº¦: {len(llm_output)} å­—ç¬¦")
                    logger.debug(f"[Orchestrator] LLM å“åº”å†…å®¹: {llm_output[:500]}...")
                except Exception as e:
                    logger.error(f"[Orchestrator] LLM call failed: {e}")

                    # è®°å½•é”™è¯¯
                    await self._monitoring.record_llm_call(
                        model=self._llm_config.get("llm_model", "unknown"),
                        tokens_used=0,
                        duration=0,
                        success=False,
                        error=e,
                    )

                    await self._publish_event("error", {
                        "message": f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
                    })
                    # è¿”å›é”™è¯¯çŠ¶æ€
                    return {
                        "agent": self.name,
                        "status": "error",
                        "error": f"LLM è°ƒç”¨å¤±è´¥: {str(e)}",
                        "thinking_chain": self.thinking_chain,
                    }

                if not llm_output or not llm_output.strip():
                    logger.warning(f"[Orchestrator] Empty LLM response")
                    # ç©ºå“åº”é‡è¯•æœºåˆ¶
                    empty_count = getattr(self, '_empty_response_count', 0) + 1
                    self._empty_response_count = empty_count
                    if empty_count >= 3:
                        error_msg = "è¿ç»­ 3 æ¬¡æ”¶åˆ°ç©ºå“åº”ï¼Œåœæ­¢å®¡è®¡"
                        await self._publish_event("error", {"message": error_msg})
                        return {
                            "agent": self.name,
                            "status": "error",
                            "error": error_msg,
                            "thinking_chain": self.thinking_chain,
                        }
                    # æç¤º LLM é‡æ–°è¾“å‡º
                    self._conversation.append(LLMMessage(role="user", content="è¯·è¾“å‡ºä½ çš„å†³ç­–ï¼šThought + Action + Action Input"))
                    continue

                # é‡ç½®ç©ºå“åº”è®¡æ•°
                self._empty_response_count = 0

                # è§£æ LLM çš„å†³ç­–
                step = self._parse_llm_response(llm_output)

                if step:
                    logger.info(f"[Orchestrator] è§£ææˆåŠŸ: action={step.action}, thought={step.thought[:50]}...")
                else:
                    logger.warning(f"[Orchestrator] è§£æå¤±è´¥ï¼Œæ— æ³•æå– Thought/Action")

                if not step:
                    # LLM è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼Œæç¤ºé‡è¯•
                    format_count = getattr(self, '_format_error_count', 0) + 1
                    self._format_error_count = format_count
                    if format_count >= 3:
                        error_msg = "è¿ç»­ 3 æ¬¡æ ¼å¼é”™è¯¯ï¼Œåœæ­¢å®¡è®¡"
                        await self._publish_event("error", {"message": error_msg})
                        return {
                            "agent": self.name,
                            "status": "error",
                            "error": error_msg,
                            "thinking_chain": self.thinking_chain,
                        }
                    await self._publish_event("thinking", {
                        "message": f"LLM è¾“å‡ºæ ¼å¼é”™è¯¯ ({format_count}/3)ï¼Œè¯·é‡æ–°è¾“å‡º"
                    })
                    self._conversation.append(LLMMessage(role="assistant", content=llm_output))
                    self._conversation.append(LLMMessage(role="user", content="è¯·æŒ‰ç…§è§„å®šæ ¼å¼è¾“å‡ºï¼šThought + Action + Action Input"))
                    continue

                # é‡ç½®æ ¼å¼é”™è¯¯è®¡æ•°
                self._format_error_count = 0

                self._steps.append(step)

                # å‘é€æ€è€ƒå†…å®¹äº‹ä»¶
                if step.thought:
                    self.think(step.thought)
                    await self._publish_event("thinking", {
                        "message": step.thought
                    })

                # æ·»åŠ  LLM å“åº”åˆ°å†å²
                self._conversation.append(LLMMessage(role="assistant", content=llm_output))

                # æ‰§è¡Œ LLM å†³å®šçš„æ“ä½œ
                if step.action == "finish":
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ‰§è¡Œäº†å¿…è¦çš„æ­¥éª¤
                    if len(self._steps) <= 2 and iteration == 1:
                        # ç¬¬ä¸€æ­¥å°±è°ƒç”¨ finishï¼Œæ‹’ç»å¹¶è¦æ±‚å…ˆè°ƒåº¦ recon
                        logger.warning(f"[Orchestrator] LLM å°è¯•åœ¨ç¬¬ä¸€æ­¥ç›´æ¥è°ƒç”¨ finishï¼Œæ‹’ç»")
                        await self._publish_event("thinking", {
                            "message": "ä¸èƒ½ç›´æ¥å®Œæˆå®¡è®¡ï¼Œå¿…é¡»å…ˆè°ƒåº¦ recon Agent"
                        })
                        self._conversation.append(LLMMessage(role="user", content="""
ä½ ä¸èƒ½ç›´æ¥è°ƒç”¨ finishã€‚å¿…é¡»æŒ‰ç…§å®¡è®¡æµç¨‹æ‰§è¡Œï¼š

1. é¦–å…ˆè°ƒç”¨ dispatch_agent è°ƒåº¦ recon Agent
2. ç„¶åæ ¹æ®ç»“æœè°ƒåº¦ analysis Agent
3. æœ€åæ‰èƒ½è°ƒç”¨ finish

è¯·é‡æ–°å¼€å§‹ï¼Œå…ˆè°ƒç”¨ recon Agentã€‚

ç¤ºä¾‹ï¼š
Thought: æˆ‘éœ€è¦å…ˆäº†è§£é¡¹ç›®çš„ç»“æ„å’ŒæŠ€æœ¯æ ˆ
Action: dispatch_agent
Action Input: {"agent": "recon", "task": "ä¾¦å¯Ÿé¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆ"}
"""))
                        continue

                    # LLM å†³å®šå®Œæˆå®¡è®¡
                    self.think("å®¡è®¡å®Œæˆï¼ŒLLM åˆ¤æ–­å®¡è®¡å·²å……åˆ†å®Œæˆ")
                    await self._publish_event("status", {
                        "status": "completed",
                        "message": f"å®¡è®¡å®Œæˆï¼Œå‘ç° {len(self._all_findings)} ä¸ªæ¼æ´"
                    })
                    final_result = step.action_input
                    break

                elif step.action == "dispatch_agent":
                    # LLM å†³å®šè°ƒåº¦å­ Agent
                    agent_name = step.action_input.get("agent", "")
                    task = step.task = step.action_input.get("task", "")

                    # æ ¹æ®agentç±»å‹è½¬æ¢å®¡è®¡é˜¶æ®µ
                    if agent_name == "recon":
                        await self._phase_manager.transition_to(AuditPhase.RECONNAISSANCE)
                        self._update_progress(15, f"å¼€å§‹ä¾¦å¯Ÿé¡¹ç›®ç»“æ„")
                    elif agent_name == "analysis":
                        await self._phase_manager.transition_to(AuditPhase.ANALYSIS)
                        self._update_progress(45, f"å¼€å§‹åˆ†ææ¼æ´")
                    elif agent_name == "verification":
                        await self._phase_manager.transition_to(AuditPhase.VERIFICATION)
                        self._update_progress(75, f"å¼€å§‹éªŒè¯æ¼æ´")

                    self.think(f"è°ƒåº¦ {agent_name} Agent: {task[:100]}")
                    await self._publish_event("action", {
                        "message": f"è°ƒåº¦ {agent_name} Agent",
                        "agent": agent_name,
                        "task": task
                    })

                    try:
                        observation = await self._dispatch_agent(step.action_input)
                        step.observation = observation

                        # æ›´æ–°è¿›åº¦å’Œé˜¶æ®µ
                        if agent_name == "recon":
                            await self._phase_manager.transition_to(AuditPhase.ANALYSIS)
                            self._update_progress(35, "ä¾¦å¯Ÿå®Œæˆï¼Œå‡†å¤‡åˆ†æ")
                        elif agent_name == "analysis":
                            # åˆ†æå®Œæˆåï¼Œå»ºè®®è¿›å…¥éªŒè¯é˜¶æ®µ
                            await self._phase_manager.transition_to(AuditPhase.VERIFICATION)
                            self._update_progress(70, "åˆ†æå®Œæˆï¼Œå‡†å¤‡éªŒè¯")

                            # æ·»åŠ æç¤ºï¼Œå‘Šè¯‰LLMæ¥ä¸‹æ¥çš„é€‰æ‹©
                            observation = f"""{observation}

---

## ğŸ“Š åˆ†æå·²å®Œæˆ

åˆ†æAgentå·²å®Œæˆä»£ç å®¡è®¡ã€‚ç°åœ¨ä½ å¯ä»¥ï¼š

1. **æŸ¥çœ‹ä¸Šè¿°åˆ†æç»“æœ**
2. **å¦‚æœå‘ç°é«˜å±æ¼æ´ï¼Œå¼ºçƒˆå»ºè®®è°ƒåº¦ `verification` Agent è¿›è¡ŒéªŒè¯**
3. å¦‚æœæ»¡æ„ï¼Œä¹Ÿå¯ä»¥è°ƒç”¨ `finish` å®Œæˆå®¡è®¡

**å»ºè®®ï¼šå¦‚æœæœ‰é«˜å±æ¼æ´ï¼Œè¯·åŠ¡å¿…éªŒè¯ï¼**
"""
                            step.observation = observation
                        
                        elif agent_name == "verification":
                            await self._phase_manager.transition_to(AuditPhase.COMPLETE)
                            self._update_progress(95, "éªŒè¯å®Œæˆï¼Œå‡†å¤‡ç”ŸæˆæŠ¥å‘Š")

                    except Exception as e:
                        logger.error(f"[Orchestrator] Sub-agent {agent_name} failed: {e}")
                        observation = f"## {agent_name} Agent æ‰§è¡Œå¤±è´¥\n\né”™è¯¯: {str(e)}"
                        step.observation = observation
                        await self._publish_event("error", {
                            "message": f"{agent_name} Agent æ‰§è¡Œå¤±è´¥: {str(e)[:100]}"
                        })

                    # å‘é€è§‚å¯Ÿäº‹ä»¶
                    self.think(f"{agent_name} Agent æ‰§è¡Œå®Œæˆ")

                elif step.action == "summarize":
                    # LLM è¦æ±‚æ±‡æ€»
                    self.think("æ±‡æ€»å½“å‰å‘ç°")
                    await self._publish_event("thinking", {
                        "message": "æ±‡æ€»å½“å‰å‘ç°"
                    })
                    observation = self._summarize_findings()
                    step.observation = observation

                else:
                    observation = f"æœªçŸ¥æ“ä½œ: {step.action}ï¼Œå¯ç”¨æ“ä½œ: dispatch_agent, summarize, finish"
                    step.observation = observation
                    await self._publish_event("thinking", {
                        "message": observation
                    })

                # æ·»åŠ è§‚å¯Ÿç»“æœåˆ°å†å²
                self._conversation.append(LLMMessage(role="user", content=f"Observation:\n{step.observation}"))

            # ç”Ÿæˆæœ€ç»ˆç»“æœ
            duration_ms = int((time.time() - start_time) * 1000)

            # æ›´æ–°è¿›åº¦åˆ° 100%
            self._update_progress(100, "å®¡è®¡å®Œæˆ")

            await self._publish_event("status", {
                "status": "completed",
                "message": f"Orchestrator å®Œæˆ: {len(self._all_findings)} ä¸ªå‘ç°, {len(self._steps)} è½®å†³ç­–"
            })

            return {
                "agent": self.name,
                "status": "success",
                "result": {
                    "findings": self._all_findings,
                    "summary": final_result or self._generate_default_summary(),
                    "steps": [
                        {
                            "thought": s.thought,
                            "action": s.action,
                            "action_input": s.action_input,
                            "observation": s.observation[:500] if s.observation else None,
                        }
                        for s in self._steps
                    ],
                },
                "thinking_chain": self.thinking_chain,
                "duration_ms": duration_ms,
                "stats": {
                    "files_scanned": self._runtime_context.get("files_scanned", 0),
                    "findings_count": len(self._all_findings),
                }
            }

        except Exception as e:
            logger.error(f"Orchestrator execution failed: {e}", exc_info=True)
            return {
                "agent": self.name,
                "status": "error",
                "error": str(e),
                "thinking_chain": self.thinking_chain
            }

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ CTX-Audit çš„ç¼–æ’ Agentï¼Œè´Ÿè´£**è‡ªä¸»**åè°ƒæ•´ä¸ªå®‰å…¨å®¡è®¡æµç¨‹ã€‚

## ä½ çš„è§’è‰²
ä½ æ˜¯æ•´ä¸ªå®¡è®¡æµç¨‹çš„**å¤§è„‘**ï¼Œä½ éœ€è¦ï¼š
1. è‡ªä¸»æ€è€ƒå’Œå†³ç­–
2. æ ¹æ®è§‚å¯Ÿç»“æœåŠ¨æ€è°ƒæ•´ç­–ç•¥
3. å†³å®šä½•æ—¶è°ƒç”¨å“ªä¸ªå­ Agent
4. åˆ¤æ–­ä½•æ—¶å®¡è®¡å®Œæˆ

## ğŸ§  æˆ˜ç•¥æ€è€ƒåè®® (Strategic Thinking Protocol) âš¡âš¡âš¡
ä½œä¸ºç¼–æ’è€…ï¼Œä½ çš„ `Thought` å¿…é¡»åŒ…å«ä»¥ä¸‹ç»´åº¦ï¼š

### 1. å…¨å±€æ€åŠ¿æ„ŸçŸ¥ (Situational Awareness)
- **å½“å‰é˜¶æ®µ**ï¼šæˆ‘ä»¬åœ¨å®¡è®¡çš„å“ªä¸ªé˜¶æ®µï¼Ÿ(ä¾¦å¯Ÿ/åˆ†æ/éªŒè¯)
- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šæˆ‘ä»¬å¯¹é¡¹ç›®ç»“æ„ã€æŠ€æœ¯æ ˆã€æ”»å‡»é¢äº†è§£å¤šå°‘ï¼Ÿæ˜¯å¦è¿˜æœ‰ç›²åŒºï¼Ÿ

### 2. å†³ç­–é€»è¾‘ (Decision Logic)
- **Why**: ä¸ºä»€ä¹ˆè¦è°ƒç”¨è¿™ä¸ª Agentï¼Ÿ(ä¾‹å¦‚ï¼š"Recon å‘ç°å¤§é‡ API ç«¯ç‚¹ï¼Œéœ€è¦ Analysis æ·±åº¦æ‰«æ")
- **Expectation**: æœŸæœ›å®ƒè¿”å›ä»€ä¹ˆï¼Ÿ(ä¾‹å¦‚ï¼š"æœŸæœ›å‘ç°æœªæˆæƒè®¿é—®æ¼æ´")

### 3. è‡ªæˆ‘åæ€ (Self-Reflection)
- æˆ‘æ˜¯å¦åœ¨é‡å¤è°ƒåº¦åŒä¸€ä¸ª Agent è€Œæ²¡æœ‰æ–°å‘ç°ï¼Ÿ
- æˆ‘æ˜¯å¦é—æ¼äº†æŸäº›é«˜å±æ–‡ä»¶ï¼Ÿ

## ä½ å¯ä»¥è°ƒåº¦çš„å­ Agent
1. **recon**: ä¿¡æ¯æ”¶é›† Agent - åˆ†æé¡¹ç›®ç»“æ„ã€æŠ€æœ¯æ ˆã€å…¥å£ç‚¹
2. **analysis**: åˆ†æ Agent - æ·±åº¦ä»£ç å®¡è®¡ã€æ¼æ´æ£€æµ‹
3. **verification**: éªŒè¯ Agent - éªŒè¯å‘ç°çš„æ¼æ´ï¼Œç”Ÿæˆ PoC

## ä½ å¯ä»¥ä½¿ç”¨çš„æ“ä½œ

### 1. è°ƒåº¦å­ Agent
```
Action: dispatch_agent
Action Input: {"agent": "recon", "task": "ä¾¦å¯Ÿé¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆ"}
```

### 2. æ±‡æ€»å‘ç°
```
Action: summarize
Action Input: {}
```

### 3. å®Œæˆå®¡è®¡
```
Action: finish
Action Input: {"conclusion": "å®¡è®¡ç»“è®º"}
```

## å·¥ä½œæ–¹å¼
æ¯ä¸€æ­¥ï¼Œä½ éœ€è¦ï¼š

1. **Thought**: ä¸¥æ ¼éµå¾ª [æˆ˜ç•¥æ€è€ƒåè®®]ã€‚åˆ†æå½“å‰çŠ¶æ€ï¼Œè¯„ä¼°å·²æœ‰çš„å‘ç°ï¼Œå†³å®šä¸‹ä¸€æ­¥çš„æœ€ä½³ç­–ç•¥ã€‚
2. **Action**: é€‰æ‹©ä¸€ä¸ªæ“ä½œ (dispatch_agent/summarize/finish)
3. **Action Input**: æä¾›æ“ä½œå‚æ•° (å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON)

## è¾“å‡ºæ ¼å¼
æ¯ä¸€æ­¥å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼ˆç¦æ­¢ä½¿ç”¨ Markdown æ ¼å¼æ ‡è®°ï¼‰ï¼š

```
Thought: 
[Situational Awareness] Recon å·²å®Œæˆï¼Œç¡®è®¤é¡¹ç›®ä¸º Python Flask åº”ç”¨ï¼Œå‘ç° 3 ä¸ª API ç«¯ç‚¹å’Œ 1 ä¸ªæ•°æ®åº“è¿æ¥æ–‡ä»¶ã€‚
[Decision Logic] ç”±äºå‘ç°äº†æ•°æ®åº“è¿æ¥ä»£ç ï¼Œå­˜åœ¨ SQL æ³¨å…¥é£é™©ï¼Œéœ€è¦è°ƒåº¦ Analysis Agent è¿›è¡Œæ·±åº¦æ‰«æã€‚
[Expectation] æœŸæœ› Analysis Agent èƒ½è¦†ç›–è¿™äº›é«˜é£é™©æ–‡ä»¶ã€‚

Action: dispatch_agent
Action Input: {"agent": "analysis", "task": "æ·±åº¦åˆ†ææ•°æ®åº“è¿æ¥å’ŒAPIæ¥å£"}
```

## âš ï¸ é‡è¦æ ¼å¼è¦æ±‚

**ç¦æ­¢ä½¿ç”¨ Markdown æ ¼å¼æ ‡è®°ï¼** ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯çº¯æ–‡æœ¬æ ¼å¼ã€‚

## å®¡è®¡æµç¨‹è¦æ±‚

è™½ç„¶ä½ éœ€è¦è‡ªä¸»å†³ç­–ï¼Œä½†é€šå¸¸éµå¾ªä»¥ä¸‹é€»è¾‘ï¼š

1. **ä¾¦å¯Ÿé˜¶æ®µ**ï¼šè°ƒç”¨ `recon` Agent äº†è§£é¡¹ç›®ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥å†æ¬¡è°ƒç”¨ã€‚
2. **åˆ†æé˜¶æ®µ**ï¼šè°ƒç”¨ `analysis` Agent è¿›è¡Œæ·±åº¦åˆ†æã€‚
   - å¦‚æœå‘ç°å¤§é‡æ¼æ´ï¼Œå¯ä»¥è€ƒè™‘åˆ†æ‰¹åˆ†æã€‚
   - å¦‚æœæ²¡æœ‰å‘ç°æ¼æ´ï¼Œä½†ä½ æ€€ç–‘æœ‰é—æ¼ï¼Œå¯ä»¥æŒ‡å®šç‰¹å®šçš„å…³æ³¨ç‚¹å†æ¬¡è°ƒç”¨ `analysis`ã€‚
3. **éªŒè¯é˜¶æ®µ**ï¼ˆæ¨èï¼‰ï¼šå¦‚æœæœ‰é«˜å±æ¼æ´ï¼Œè°ƒç”¨ `verification` Agent è¿›è¡ŒéªŒè¯ã€‚
4. **å®Œæˆ**ï¼šè°ƒç”¨ `finish` å®Œæˆå®¡è®¡ã€‚

**é‡è¦ï¼š**
- å¿…é¡»å…ˆä¾¦å¯Ÿ (recon)ã€‚
- ä¸è¦æ€¥äº finishï¼Œç¡®ä¿å·²å……åˆ†æŒ–æ˜æ½œåœ¨æ¼æ´ã€‚
- åªæœ‰åœ¨ç¡®è®¤æ²¡æœ‰æ›´å¤šå·¥ä½œéœ€è¦åšæ—¶ï¼Œæ‰è°ƒç”¨ finishã€‚
- Action Input å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚

## ç¤ºä¾‹æµç¨‹

```
Thought: æˆ‘éœ€è¦å…ˆäº†è§£é¡¹ç›®çš„ç»“æ„å’ŒæŠ€æœ¯æ ˆï¼Œä»¥ä¾¿è¿›è¡Œåç»­çš„å®‰å…¨å®¡è®¡
Action: dispatch_agent
Action Input: {"agent": "recon", "task": "åˆ†æé¡¹ç›®ç»“æ„ã€æŠ€æœ¯æ ˆå’Œå…¥å£ç‚¹"}

Observation: [recon ç»“æœ...]

Thought: 
[Situational Awareness] é¡¹ç›®æ˜¯ Python Flask åº”ç”¨ï¼Œå‘ç°äº†ä¸€äº›é«˜é£é™©åŒºåŸŸã€‚
[Decision Logic] ç°åœ¨æˆ‘éœ€è¦å¯¹è¿™äº›åŒºåŸŸè¿›è¡Œæ·±åº¦åˆ†æã€‚

Action: dispatch_agent
Action Input: {"agent": "analysis", "task": "æ·±åº¦åˆ†æé«˜é£é™©åŒºåŸŸçš„ä»£ç å®‰å…¨é—®é¢˜"}

Observation: [analysis ç»“æœ...]

Thought: åˆ†æå‘ç°äº†ä¸€ä¸ª SQL æ³¨å…¥æ¼æ´ï¼Œæˆ‘éœ€è¦éªŒè¯å®ƒæ˜¯å¦çœŸå®å­˜åœ¨
Action: dispatch_agent
Action Input: {"agent": "verification", "task": "éªŒè¯ SQL æ³¨å…¥æ¼æ´"}

Observation: [verification ç»“æœ...]

Thought: å·²å®ŒæˆéªŒè¯ï¼Œæ¼æ´ç¡®è®¤ä¸ºçœŸå®ã€‚å®¡è®¡å·¥ä½œå·²ç»å……åˆ†å®Œæˆ
Action: finish
Action Input: {"conclusion": "å®¡è®¡ç»“è®º"}
```

ç°åœ¨å¼€å§‹å®¡è®¡ï¼Œè¯·å…ˆè°ƒç”¨ recon Agentï¼"""

    def _format_initial_message(self, context: Dict[str, Any]) -> str:
        """æ„å»ºåˆå§‹æ¶ˆæ¯"""
        return f"""è¯·å¼€å§‹å¯¹ä»¥ä¸‹é¡¹ç›®è¿›è¡Œå®‰å…¨å®¡è®¡ã€‚

## é¡¹ç›®ä¿¡æ¯
- Project ID: {context.get("project_id", "unknown")}
- Audit ID: {context.get("audit_id", "unknown")}
- Audit Type: {context.get("audit_type", "quick")}

## å¯ç”¨å­ Agent
- recon: ä¿¡æ¯æ”¶é›† Agentï¼Œç”¨äºåˆ†æé¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆ
- analysis: åˆ†æ Agentï¼Œç”¨äºæ·±åº¦ä»£ç å®¡è®¡å’Œæ¼æ´æ£€æµ‹
- verification: éªŒè¯ Agentï¼Œç”¨äºéªŒè¯å‘ç°çš„æ¼æ´

## âš ï¸ é‡è¦æç¤º
æ¨èçš„æ‰§è¡Œæ­¥éª¤ï¼š
1. **é¦–å…ˆ**è°ƒç”¨ recon Agent äº†è§£é¡¹ç›®
2. **ç„¶å**è°ƒç”¨ analysis Agent è¿›è¡Œåˆ†æ
3. **å¯é€‰**è°ƒç”¨ verification Agent éªŒè¯é«˜å±æ¼æ´
4. **æœ€å**è°ƒç”¨ finish å®Œæˆå®¡è®¡

**ä¸èƒ½ç›´æ¥è°ƒç”¨ finishï¼å¿…é¡»å…ˆè°ƒåº¦ recon Agentï¼**

è¯·ç«‹å³å¼€å§‹ï¼šé¦–å…ˆè¾“å‡ºä½ çš„æ€è€ƒï¼Œç„¶åè°ƒç”¨ dispatch_agent è°ƒåº¦ recon Agentã€‚

ç¤ºä¾‹ï¼š
Thought: æˆ‘éœ€è¦å…ˆäº†è§£é¡¹ç›®çš„ç»“æ„å’ŒæŠ€æœ¯æ ˆ
Action: dispatch_agent
Action Input: {{"agent": "recon", "task": "ä¾¦å¯Ÿé¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆ"}}"""


    def _parse_llm_response(self, response: str) -> Optional[AgentStep]:
        """è§£æ LLM å“åº”"""
        # é¢„å¤„ç† - ç§»é™¤ Markdown æ ¼å¼æ ‡è®°
        cleaned_response = response
        cleaned_response = re.sub(r'\*\*Action:\*\*', 'Action:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Action Input:\*\*', 'Action Input:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Thought:\*\*', 'Thought:', cleaned_response)

        # æå– Thought
        thought_match = re.search(r'Thought:\s*(.*?)(?=Action:|$)', cleaned_response, re.DOTALL)
        thought = thought_match.group(1).strip() if thought_match else ""

        # æå– Action
        action_match = re.search(r'Action:\s*(\w+)', cleaned_response)
        if not action_match:
            return None
        action = action_match.group(1).strip()

        # æå– Action Input
        input_match = re.search(r'Action Input:\s*(.*?)(?=Thought:|Observation:|$)', cleaned_response, re.DOTALL)
        if not input_match:
            return None

        input_text = input_match.group(1).strip()
        # ç§»é™¤ markdown ä»£ç å—
        input_text = re.sub(r'```json\s*', '', input_text)
        input_text = re.sub(r'```\s*', '', input_text)

        try:
            action_input = json.loads(input_text)
        except json.JSONDecodeError:
            # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•æå–åŸå§‹æ–‡æœ¬
            action_input = {"raw": input_text}

        return AgentStep(
            thought=thought,
            action=action,
            action_input=action_input,
        )

    async def _dispatch_agent(self, params: Dict[str, Any]) -> str:
        """è°ƒåº¦å­ Agent"""
        agent_name = params.get("agent", "")
        task = params.get("task", "")

        logger.info(f"[Orchestrator] Dispatching {agent_name} Agent: {task[:50]}...")

        # æ£€æŸ¥æ˜¯å¦é‡å¤è°ƒåº¦åŒä¸€ä¸ª Agent
        dispatch_count = self._dispatched_tasks.get(agent_name, 0)
        # æ”¾å®½é‡å¤è°ƒåº¦é™åˆ¶ï¼Œå…è®¸ Analysis å¤šæ¬¡è¿è¡Œ
        if dispatch_count >= 3:
            return f"""## é‡å¤è°ƒåº¦è­¦å‘Š

ä½ å·²ç»è°ƒåº¦ {agent_name} Agent {dispatch_count} æ¬¡äº†ã€‚

å¦‚æœä¹‹å‰çš„è°ƒåº¦æ²¡æœ‰è¿”å›æœ‰ç”¨çš„ç»“æœï¼Œè¯·è€ƒè™‘ï¼š
1. å°è¯•è°ƒåº¦å…¶ä»– Agent
2. ä½¿ç”¨ finish æ“ä½œç»“æŸå®¡è®¡å¹¶æ±‡æ€»å·²æœ‰å‘ç°

å½“å‰å·²æ”¶é›†çš„å‘ç°æ•°é‡: {len(self._all_findings)}
"""

        self._dispatched_tasks[agent_name] = dispatch_count + 1

        try:
            # åˆ›å»ºå­ Agent
            agent_id = await agent_graph_controller.create_agent(
                agent_type=agent_name,
                task=task,
                parent_id=self.agent_id,
            )

            # è·å– agent å®ä¾‹
            agent = await agent_registry.get_agent_instance(agent_id)
            if not agent:
                return f"## è°ƒåº¦å¤±è´¥\n\né”™è¯¯: æ— æ³•è·å– Agent å®ä¾‹: {agent_id}"

            # å‡†å¤‡ LLM é…ç½®å‚æ•°
            llm_params = {}
            
            # 1. ä» Orchestrator é…ç½®è·å–
            if self._llm_config.get("llm_provider"):
                llm_params["llm_provider"] = self._llm_config.get("llm_provider")
            if self._llm_config.get("llm_model"):
                llm_params["llm_model"] = self._llm_config.get("llm_model")
            if self._llm_config.get("api_key"):
                llm_params["api_key"] = self._llm_config.get("api_key")
            if self._llm_config.get("base_url"):
                llm_params["base_url"] = self._llm_config.get("base_url")
                
            # 2. å¦‚æœç¼ºå¤±ï¼Œå°è¯•ä»è¿è¡Œæ—¶ä¸Šä¸‹æ–‡çš„ config è·å– (Global Config)
            global_config = self._runtime_context.get("config", {})
            if not llm_params.get("api_key") and global_config.get("api_key"):
                llm_params["api_key"] = global_config.get("api_key")
                # Also check for provider/model in global config
                if not llm_params.get("llm_provider") and global_config.get("llm_provider"):
                    llm_params["llm_provider"] = global_config.get("llm_provider")
                if not llm_params.get("llm_model") and global_config.get("llm_model"):
                    llm_params["llm_model"] = global_config.get("llm_model")

            # Log dispatch info (mask key)
            has_key = bool(llm_params.get("api_key"))
            logger.info(f"[Orchestrator] Dispatching {agent_name} with LLM config: provider={llm_params.get('llm_provider')}, has_key={has_key}")

            # æ‰§è¡Œå­ Agent
            result_data = await agent.run({
                "audit_id": self._runtime_context.get("audit_id"),
                "project_id": self._runtime_context.get("project_id"),
                "project_path": self._runtime_context.get("project_path"),
                "task": task,
                # ä¼ é€’å·²æœ‰çš„å‘ç°ç»™éªŒè¯ Agent
                "findings": self._all_findings if agent_name == "verification" else [],
                # ä¼ é€’ä¹‹å‰ Agent çš„ç»“æœ
                **self._agent_results,
                # ä¼ é€’ LLM é…ç½®ç»™å­ Agent
                **llm_params
            })

            # æå–ç»“æœ
            status = result_data.get("status")
            result = result_data.get("result", {})
            
            if status == "error":
                return f"## {agent_name} Agent æ‰§è¡Œå‡ºé”™\n\né”™è¯¯: {result_data.get('error')}"

            # ------------------------------------------------------------------
            # CRITICAL FIX: æ”¶é›†å­ Agent çš„å‘ç°
            # ------------------------------------------------------------------
            new_findings = []
            
            # Recon Agent è¿”å› findings åœ¨ tool_findings, dataflow_findings ç­‰å­—æ®µ
            if agent_name == "recon":
                if isinstance(result, dict):
                    # æå–å·¥å…·å‘ç°
                    if "tool_findings" in result:
                        new_findings.extend(result["tool_findings"])
                    # æå–æ•°æ®æµå‘ç°
                    if "dataflow_findings" in result:
                        new_findings.extend(result["dataflow_findings"])
            
            # Analysis Agent é€šå¸¸ç›´æ¥è¿”å› findings åˆ—è¡¨æˆ–åŒ…å« findings çš„å­—å…¸
            elif agent_name == "analysis":
                if isinstance(result, dict):
                    if "findings" in result:
                        new_findings.extend(result["findings"])
                elif isinstance(result, list):
                    new_findings.extend(result)

            # Verification Agent è¿”å›éªŒè¯åçš„ findings
            elif agent_name == "verification":
                # éªŒè¯ Agent ä¸äº§ç”Ÿæ–°æ¼æ´ï¼Œè€Œæ˜¯æ›´æ–°ç°æœ‰æ¼æ´çš„çŠ¶æ€
                # è¿™é‡Œæˆ‘ä»¬å¯ä»¥è·å–éªŒè¯ç»“æœæŠ¥å‘Š
                verified_results = result.get("verified", [])
                # æ›´æ–° _all_findings ä¸­çš„éªŒè¯çŠ¶æ€
                for v_res in verified_results:
                    f_id = v_res.get("finding_id")
                    is_verified = v_res.get("verified", False)
                    for finding in self._all_findings:
                        if finding.get("id") == f_id:
                            finding["verified"] = is_verified
                            finding["verification_evidence"] = v_res.get("evidence")
                            finding["poc_code"] = v_res.get("poc_code")

            # å°†æ–°å‘ç°æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­ (å»é‡)
            added_count = 0
            existing_ids = {f.get("id") for f in self._all_findings if f.get("id")}
            
            for f in new_findings:
                # ç¡®ä¿æ¯ä¸ª finding éƒ½æœ‰ ID
                if not f.get("id"):
                    import uuid
                    f["id"] = str(uuid.uuid4())
                
                if f.get("id") not in existing_ids:
                    self._all_findings.append(f)
                    existing_ids.add(f.get("id"))
                    added_count += 1
            
            logger.info(f"[Orchestrator] ä» {agent_name} æ”¶é›†åˆ° {added_count} ä¸ªæ–°å‘ç° (æ€»è®¡: {len(self._all_findings)})")
            
            # å®æ—¶é€šçŸ¥å‰ç«¯æœ‰æ–°å‘ç°
            if added_count > 0:
                await self._publish_event("findings_detected", {
                    "message": f"ç”± {agent_name} å‘ç° {added_count} ä¸ªæ–°é—®é¢˜",
                    "count": added_count,
                    "total": len(self._all_findings),
                    "agent": agent_name
                })
            
            # ------------------------------------------------------------------

            # ä¿å­˜ Agent ç»“æœ
            self._agent_results[agent_name] = result

            # æ ¼å¼åŒ–è§‚å¯Ÿç»“æœä¾› LLM é˜…è¯»
            if agent_name == "recon":
                summary = f"å‘ç° {len(new_findings)} ä¸ªæ½œåœ¨é—®é¢˜ã€‚é¡¹ç›®æŠ€æœ¯æ ˆ: {result.get('tech_stack', {}).get('languages', [])}"
            elif agent_name == "analysis":
                summary = f"åˆ†æå®Œæˆï¼Œå‘ç° {len(new_findings)} ä¸ªæ¼æ´ã€‚"
            elif agent_name == "verification":
                summary = f"éªŒè¯å®Œæˆã€‚ç¡®è®¤ {result.get('total_verified', 0)} ä¸ªæ¼æ´ï¼Œæ’é™¤ {result.get('total_false_positives', 0)} ä¸ªè¯¯æŠ¥ã€‚"
            else:
                summary = "æ‰§è¡Œå®Œæˆ"

            return f"""## {agent_name} æ‰§è¡ŒæˆåŠŸ

{summary}

### è¯¦ç»†ç»“æœæ‘˜è¦
- æ–°å¢å‘ç°: {added_count}
- å½“å‰æ€»å‘ç°: {len(self._all_findings)}

(å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°ä¸Šä¸‹æ–‡)
"""

        except Exception as e:
            logger.error(f"è°ƒåº¦ {agent_name} å¤±è´¥: {e}", exc_info=True)
            return f"è°ƒåº¦å¤±è´¥: {str(e)}"

    def _normalize_finding(self, finding: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–å‘ç°æ ¼å¼"""
        normalized = dict(finding)

        # å¤„ç† file -> file_path
        if "file" in normalized and "file_path" not in normalized:
            normalized["file_path"] = normalized["file"]

        # å¤„ç† line -> line_start
        if "line" in normalized and "line_start" not in normalized:
            normalized["line_start"] = normalized["line"]

        # å¤„ç† type -> vulnerability_type
        if "type" in normalized and "vulnerability_type" not in normalized:
            type_val = normalized["type"]
            if type_val and type_val.lower() not in ["vulnerability", "finding", "issue"]:
                normalized["vulnerability_type"] = type_val

        # ç¡®ä¿ severity å­˜åœ¨
        if "severity" not in normalized:
            normalized["severity"] = "medium"

        # ç”Ÿæˆ title å¦‚æœä¸å­˜åœ¨
        if "title" not in normalized:
            vuln_type = normalized.get("vulnerability_type", "Unknown")
            file_path = normalized.get("file_path", "")
            if file_path:
                import os
                normalized["title"] = f"{vuln_type.replace('_', ' ').title()} in {os.path.basename(file_path)}"
            else:
                normalized["title"] = f"{vuln_type.replace('_', ' ').title()} Vulnerability"

        return normalized

    def _summarize_findings(self) -> str:
        """æ±‡æ€»å½“å‰å‘ç°"""
        if not self._all_findings:
            return "ç›®å‰è¿˜æ²¡æœ‰å‘ç°ä»»ä½•æ¼æ´ã€‚"

        # ç»Ÿè®¡
        severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        type_counts = {}

        for f in self._all_findings:
            if not isinstance(f, dict):
                continue

            sev = f.get("severity", "low")
            severity_counts[sev] = severity_counts.get(sev, 0) + 1

            vtype = f.get("vulnerability_type", "other")
            type_counts[vtype] = type_counts.get(vtype, 0) + 1

        summary = f"""## å½“å‰å‘ç°æ±‡æ€»

**æ€»è®¡**: {len(self._all_findings)} ä¸ªæ¼æ´

### ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
- Critical: {severity_counts['critical']}
- High: {severity_counts['high']}
- Medium: {severity_counts['medium']}
- Low: {severity_counts['low']}

### æ¼æ´ç±»å‹åˆ†å¸ƒ
"""
        for vtype, count in type_counts.items():
            summary += f"- {vtype}: {count}\n"

        summary += "\n### è¯¦ç»†åˆ—è¡¨\n"
        for i, f in enumerate(self._all_findings):
            if isinstance(f, dict):
                summary += f"{i+1}. [{f.get('severity')}] {f.get('title')} ({f.get('file_path')})\n"

        return summary

    def _generate_default_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆé»˜è®¤æ‘˜è¦"""
        severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}

        for f in self._all_findings:
            if isinstance(f, dict):
                sev = f.get("severity", "low")
                severity_counts[sev] = severity_counts.get(sev, 0) + 1

        return {
            "total_findings": len(self._all_findings),
            "severity_distribution": severity_counts,
            "conclusion": "å®¡è®¡å®Œæˆ",
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
orchestrator_agent = OrchestratorAgent()
